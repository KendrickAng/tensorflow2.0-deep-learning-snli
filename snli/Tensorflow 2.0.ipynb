{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Hyperparameters and global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{ 'batch_size': 20,\n",
      "  'buffer_size': 10000,\n",
      "  'config_file': 'snli.config',\n",
      "  'embedding_size': 300,\n",
      "  'hidden_length': 64,\n",
      "  'max_features': 50000,\n",
      "  'max_hypothesis_length': 30,\n",
      "  'max_premise_length': 30,\n",
      "  'num_epochs': 5,\n",
      "  'snli_link': 'https://nlp.stanford.edu/projects/snli/snli_1.0.zip',\n",
      "  'snli_testfilename': 'snli_1.0_test.txt',\n",
      "  'snli_trainfilename': 'snli_1.0_train.txt',\n",
      "  'snli_validatefilename': 'snli_1.0_dev.txt',\n",
      "  'snli_zipfilename': 'snli_1.0.zip',\n",
      "  'word_embeddings_link': 'http://nlp.stanford.edu/data/glove.6B.zip',\n",
      "  'word_embeddings_txtfilename': 'glove.6B.50d.txt',\n",
      "  'word_embeddings_zipfilename': 'glove.6B.zip'}\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import argparse\n",
    "import tqdm\n",
    "import logging\n",
    "import pprint # pretty print python objects\n",
    "import sys\n",
    "import os\n",
    "\n",
    "FLAGS = {\n",
    "    \"config_file\": \"snli.config\",\n",
    "    \"buffer_size\": 10000,\n",
    "    \"batch_size\": 20,\n",
    "    \"snli_link\": \"https://nlp.stanford.edu/projects/snli/snli_1.0.zip\",\n",
    "    \"snli_zipfilename\": \"snli_1.0.zip\",\n",
    "    \"snli_trainfilename\": \"snli_1.0_train.txt\",\n",
    "    \"snli_validatefilename\": \"snli_1.0_dev.txt\",\n",
    "    \"snli_testfilename\": \"snli_1.0_test.txt\",\n",
    "    \"word_embeddings_link\": \"http://nlp.stanford.edu/data/glove.6B.zip\",\n",
    "    \"word_embeddings_zipfilename\": \"glove.6B.zip\",\n",
    "    \"word_embeddings_txtfilename\": \"glove.6B.50d.txt\",\n",
    "    \"max_premise_length\": 30,\n",
    "    \"max_hypothesis_length\": 30,\n",
    "    \"batch_size\": 128,\n",
    "    \"hidden_length\": 64,\n",
    "    \"embedding_size\": 50, # 50 dim embeddings\n",
    "    \"max_features\": 50000,\n",
    "    \"num_epochs\": 5\n",
    "}\n",
    "\n",
    "def create_logger():\n",
    "    log = logging.getLogger() # root logger\n",
    "    log.setLevel(logging.DEBUG)\n",
    "    formatter = logging.Formatter(fmt=\"%(asctime)s : %(levelname)s %(message)s\")\n",
    "    handler = logging.StreamHandler()\n",
    "    handler.setFormatter(formatter)\n",
    "    log.addHandler(handler)\n",
    "    return logging.getLogger()\n",
    "\n",
    "pp = pprint.PrettyPrinter(indent=2)\n",
    "logger = create_logger()\n",
    "glove_wordmap = {}\n",
    "glove_wordmap_size = 0\n",
    "\n",
    "print(pp.pformat(FLAGS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Snli corpus file already downloaded. Extracting...\n",
      "snli_1.0_train.txt already extracted.\n",
      "\n",
      "snli_1.0_dev.txt already extracted.\n",
      "\n",
      "snli_1.0_dev.txt already extracted.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load dataset manually\n",
    "def prepare_snli_corpus():\n",
    "    snli_link = FLAGS['snli_link']\n",
    "    snli_zip_file = FLAGS['snli_zipfilename']\n",
    "    snli_train_file = FLAGS['snli_trainfilename']\n",
    "    snli_validate_file = FLAGS['snli_validatefilename']\n",
    "    snli_test_file = FLAGS['snli_testfilename']\n",
    "    \n",
    "    if (not os.path.isfile(snli_zip_file)):\n",
    "        print(\"Snli corpus not found. Downloading from site...\")\n",
    "        import urllib.request\n",
    "        # download glove zip file\n",
    "        urllib.request.urlretrieve(snli_link, snli_zip_file)\n",
    "    print(\"Snli corpus file already downloaded. Extracting...\")\n",
    "    # extract train, validate and test files\n",
    "    if (not os.path.isfile(snli_train_file)):\n",
    "        unzip_single_file(snli_zip_file, snli_train_file)\n",
    "        print(\"Extracted {}\\n\".format(snli_validate_file))\n",
    "    else:\n",
    "        print(\"{} already extracted.\\n\".format(snli_train_file))\n",
    "    if (not os.path.isfile(snli_validate_file)):\n",
    "        unzip_single_file(snli_zip_file, snli_validate_file)\n",
    "        print(\"Extracted {}\\n\".format(snli_validate_file))\n",
    "    else:\n",
    "        print(\"{} already extracted.\\n\".format(snli_validate_file))\n",
    "    if (not os.path.isfile(snli_test_file)):\n",
    "        unzip_single_file(snli_zip_file, snli_test_file)\n",
    "        print(\"Extracted {}\\n\".format(snli_test_file))\n",
    "    else:\n",
    "        print(\"{} already extracted.\\n\".format(snli_validate_file))\n",
    "    return\n",
    "\n",
    "def prepare_glove_embeddings():\n",
    "    glove_link = FLAGS['word_embeddings_link']\n",
    "    glove_zip_file = FLAGS['word_embeddings_zipfilename']\n",
    "    glove_text_file = FLAGS['word_embeddings_txtfilename']\n",
    "    \n",
    "    if (not os.path.isfile(glove_zip_file) and not os.path.isfile(glove_text_file)):\n",
    "        print(\"Glove embeddings not found. Downloading from site...\")\n",
    "        import urllib.request\n",
    "        # download glove zip file\n",
    "        urllib.request.urlretrieve(glove_link, glove_zip_file)\n",
    "        print(\"Glove embeddings file downloaded.\")\n",
    "        # extract zip to text file\n",
    "        unzip_single_file(glove_zip_file, glove_text_file)\n",
    "    return\n",
    "\n",
    "def unzip_single_file(zip_file_name, output_file_name):\n",
    "    \"\"\"\n",
    "    If the outfile exists, don't recreate, else create from zipfile\n",
    "    \"\"\"\n",
    "    if not os.path.isfile(output_file_name):\n",
    "        import zipfile\n",
    "        print(\"Unzipping glove embeddings {}..\".format(zip_file_name))\n",
    "        with open(output_file_name, \"wb\") as out_file:\n",
    "            with zipfile.ZipFile(zip_file_name) as zipped:\n",
    "                for info in zipped.infolist():\n",
    "                    if output_file_name in info.filename:\n",
    "                        with zipped.open(info) as requested_file:\n",
    "                            out_file.write(requested_file.read())\n",
    "                            print(\"Glove embeddings unzipped to {}\".format(output_file_name))\n",
    "                            return\n",
    "    return\n",
    "\n",
    "prepare_snli_corpus()\n",
    "prepare_glove_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample word \"the\" with features array([ 4.1800e-01,  2.4968e-01, -4.1242e-01,  1.2170e-01,  3.4527e-01,\n",
      "       -4.4457e-02, -4.9688e-01, -1.7862e-01, -6.6023e-04, -6.5660e-01,\n",
      "        2.7843e-01, -1.4767e-01, -5.5677e-01,  1.4658e-01, -9.5095e-03,\n",
      "        1.1658e-02,  1.0204e-01, -1.2792e-01, -8.4430e-01, -1.2181e-01,\n",
      "       -1.6801e-02, -3.3279e-01, -1.5520e-01, -2.3131e-01, -1.9181e-01,\n",
      "       -1.8823e+00, -7.6746e-01,  9.9051e-02, -4.2125e-01, -1.9526e-01,\n",
      "        4.0071e+00, -1.8594e-01, -5.2287e-01, -3.1681e-01,  5.9213e-04,\n",
      "        7.4449e-03,  1.7778e-01, -1.5897e-01,  1.2041e-02, -5.4223e-02,\n",
      "       -2.9871e-01, -1.5749e-01, -3.4758e-01, -4.5637e-02, -4.4251e-01,\n",
      "        1.8785e-01,  2.7849e-03, -1.8411e-01, -1.1514e-01, -7.8581e-01],\n",
      "      dtype=float32)\n",
      "Glove wordmap populated, found 400000 vectors\n",
      "\n",
      "Preprocessing snli data & parsing to arrays...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "550152it [00:54, 10183.05it/s]\n"
     ]
    }
   ],
   "source": [
    "def sentence2sequence(sentence):\n",
    "    '''\n",
    "    Turns an input sentence into a (n, d) matrix. \n",
    "    n is the number of tokens in the sentence.\n",
    "    d is the number of dimensions each word vector has.\n",
    "    '''\n",
    "    tokens = None\n",
    "\n",
    "    try:\n",
    "        tokens = sentence.decode().lower().split(\" \")\n",
    "    except AttributeError: # not byte-encoded\n",
    "        tokens = sentence.lower().split(\" \")\n",
    "    rows = []\n",
    "    words = []\n",
    "    for token in tokens: # each token is a word in the sentence\n",
    "        i = len(token)\n",
    "        while len(token) > 0 and i > 0:\n",
    "            word = token[:i]\n",
    "            if word in glove_wordmap:\n",
    "                rows.append(glove_wordmap[word])\n",
    "                words.append(word)\n",
    "                token = token[i:]\n",
    "                i = len(token)\n",
    "            else:\n",
    "                # no such word, add keep reducing until we find a word\n",
    "                i = i - 1\n",
    "    return { \"words\": words, \"rows\": rows }\n",
    "\n",
    "def sentence_score_setup(row):\n",
    "    convert_dict = {\n",
    "        'entailment': 0,\n",
    "        'neutral': 1,\n",
    "        'contradiction': 2\n",
    "    }\n",
    "    score = np.zeros((3,1))\n",
    "    for x in range(1,6):\n",
    "        tag = row[\"label\"+str(x)]\n",
    "        if tag in convert_dict: \n",
    "            score[convert_dict[tag]] += 1\n",
    "    return score / (1.0 * np.sum(score)) # return normalised np array\n",
    "\n",
    "def load_glove_embeddings():\n",
    "    global glove_wordmap\n",
    "    global glove_wordmap_size\n",
    "\n",
    "    glove_text_file = FLAGS['word_embeddings_txtfilename']\n",
    "    printOne = True    \n",
    "\n",
    "    with open(glove_text_file, \"r\", encoding='utf-8') as glove:\n",
    "        for line in glove:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            # tensorflow only accepts arrays, not python lists\n",
    "            featuresMatrix = np.asarray(values[1:], dtype='float32')\n",
    "            # print a sample word with feature matrix\n",
    "            if printOne:\n",
    "                printOne = False\n",
    "                print(\"Sample word \\\"{}\\\" with features {}\".format(word, pp.pformat(featuresMatrix)))\n",
    "            glove_wordmap[word] = featuresMatrix\n",
    "    glove_wordmap_size = len(glove_wordmap)\n",
    "    print(\"Glove wordmap populated, found %s vectors\\n\" % glove_wordmap_size)\n",
    "    \n",
    "def load_snli_data(filename):\n",
    "    if not os.path.isfile(filename):\n",
    "        print(\"ERROR: FILE NOT FOUND. EXITING...\")\n",
    "    else:\n",
    "        print(\"Preprocessing snli data & parsing to arrays...\")\n",
    "        import csv\n",
    "        with open(filename, \"r\", encoding='utf-8') as data:\n",
    "            train = csv.DictReader(data, delimiter='\\t')\n",
    "            premise_embeds = []\n",
    "            hypothesis_embeds = []\n",
    "            labels = []\n",
    "            scores = []\n",
    "            for row in tqdm.tqdm(iterable=train):\n",
    "                premise_embeds.append(\n",
    "                    np.vstack(sentence2sequence(row[\"sentence1\"].lower())[\"rows\"]))\n",
    "                hypothesis_embeds.append(\n",
    "                    np.vstack(sentence2sequence(row[\"sentence2\"].lower())[\"rows\"]))\n",
    "                labels.append(row[\"gold_label\"])\n",
    "                scores.append(sentence_score_setup(row))\n",
    "                # print(\"Sample data piece: {}\".format(pp.pformat(row)))\n",
    "                # print(pp.pformat(sentence2sequence(row['sentence1'].lower())['rows']))\n",
    "                # print(np.vstack(pp.pformat(sentence2sequence(row['sentence1'].lower())['rows'])))\n",
    "                # print(pp.pformat(sentence_score_setup(row)))\n",
    "            return (premise_embeds, hypothesis_embeds), labels, scores        \n",
    "    \n",
    "load_glove_embeddings()\n",
    "# 550152 rows processed\n",
    "data_features_tuple, labels, scores = load_snli_data(FLAGS['snli_trainfilename'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "550152\n"
     ]
    }
   ],
   "source": [
    "input_length = FLAGS['max_premise_length'] + FLAGS['max_hypothesis_length'] # first layer length\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

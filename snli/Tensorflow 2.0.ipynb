{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load Hyperparameters and global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{ 'batch_size': 128,\n",
      "  'buffer_size': 10000,\n",
      "  'config_file': 'snli.config',\n",
      "  'embedding_size': 50,\n",
      "  'hidden_length': 64,\n",
      "  'max_data_items': 50000,\n",
      "  'max_features': 50000,\n",
      "  'max_hypothesis_length': 30,\n",
      "  'max_premise_length': 30,\n",
      "  'num_epochs': 5,\n",
      "  'snli_link': 'https://nlp.stanford.edu/projects/snli/snli_1.0.zip',\n",
      "  'snli_testfilename': 'snli_1.0_test.txt',\n",
      "  'snli_trainfilename': 'snli_1.0_train.txt',\n",
      "  'snli_validatefilename': 'snli_1.0_dev.txt',\n",
      "  'snli_zipfilename': 'snli_1.0.zip',\n",
      "  'word_embeddings_link': 'http://nlp.stanford.edu/data/glove.6B.zip',\n",
      "  'word_embeddings_txtfilename': 'glove.6B.50d.txt',\n",
      "  'word_embeddings_zipfilename': 'glove.6B.zip'}\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import argparse\n",
    "import tqdm\n",
    "import logging\n",
    "import pprint # pretty print python objects\n",
    "import sys\n",
    "import os\n",
    "\n",
    "FLAGS = {\n",
    "    \"config_file\": \"snli.config\",\n",
    "    \"buffer_size\": 10000,\n",
    "    \"max_data_items\": 50000,\n",
    "    \"snli_link\": \"https://nlp.stanford.edu/projects/snli/snli_1.0.zip\",\n",
    "    \"snli_zipfilename\": \"snli_1.0.zip\",\n",
    "    \"snli_trainfilename\": \"snli_1.0_train.txt\",\n",
    "    \"snli_validatefilename\": \"snli_1.0_dev.txt\",\n",
    "    \"snli_testfilename\": \"snli_1.0_test.txt\",\n",
    "    \"word_embeddings_link\": \"http://nlp.stanford.edu/data/glove.6B.zip\",\n",
    "    \"word_embeddings_zipfilename\": \"glove.6B.zip\",\n",
    "    \"word_embeddings_txtfilename\": \"glove.6B.50d.txt\",\n",
    "    \"max_premise_length\": 30,\n",
    "    \"max_hypothesis_length\": 30,\n",
    "    \"batch_size\": 128,\n",
    "    \"hidden_length\": 64,\n",
    "    \"embedding_size\": 50, # 50 dim embeddings\n",
    "    \"max_features\": 50000,\n",
    "    \"num_epochs\": 5\n",
    "}\n",
    "\n",
    "def create_logger():\n",
    "    log = logging.getLogger() # root logger\n",
    "    log.setLevel(logging.DEBUG)\n",
    "    formatter = logging.Formatter(fmt=\"%(asctime)s : %(levelname)s %(message)s\")\n",
    "    handler = logging.StreamHandler()\n",
    "    handler.setFormatter(formatter)\n",
    "    log.addHandler(handler)\n",
    "    return logging.getLogger()\n",
    "\n",
    "pp = pprint.PrettyPrinter(indent=2)\n",
    "logger = create_logger()\n",
    "glove_wordmap = {}\n",
    "glove_wordmap_size = 0\n",
    "\n",
    "print(pp.pformat(FLAGS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Snli corpus file already downloaded. Extracting...\n",
      "snli_1.0_train.txt already extracted.\n",
      "\n",
      "snli_1.0_dev.txt already extracted.\n",
      "\n",
      "snli_1.0_dev.txt already extracted.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load dataset manually\n",
    "def prepare_snli_corpus():\n",
    "    snli_link = FLAGS['snli_link']\n",
    "    snli_zip_file = FLAGS['snli_zipfilename']\n",
    "    snli_train_file = FLAGS['snli_trainfilename']\n",
    "    snli_validate_file = FLAGS['snli_validatefilename']\n",
    "    snli_test_file = FLAGS['snli_testfilename']\n",
    "    \n",
    "    if (not os.path.isfile(snli_zip_file)):\n",
    "        print(\"Snli corpus not found. Downloading from site...\")\n",
    "        import urllib.request\n",
    "        # download glove zip file\n",
    "        urllib.request.urlretrieve(snli_link, snli_zip_file)\n",
    "    print(\"Snli corpus file already downloaded. Extracting...\")\n",
    "    # extract train, validate and test files\n",
    "    if (not os.path.isfile(snli_train_file)):\n",
    "        unzip_single_file(snli_zip_file, snli_train_file)\n",
    "        print(\"Extracted {}\\n\".format(snli_validate_file))\n",
    "    else:\n",
    "        print(\"{} already extracted.\\n\".format(snli_train_file))\n",
    "    if (not os.path.isfile(snli_validate_file)):\n",
    "        unzip_single_file(snli_zip_file, snli_validate_file)\n",
    "        print(\"Extracted {}\\n\".format(snli_validate_file))\n",
    "    else:\n",
    "        print(\"{} already extracted.\\n\".format(snli_validate_file))\n",
    "    if (not os.path.isfile(snli_test_file)):\n",
    "        unzip_single_file(snli_zip_file, snli_test_file)\n",
    "        print(\"Extracted {}\\n\".format(snli_test_file))\n",
    "    else:\n",
    "        print(\"{} already extracted.\\n\".format(snli_validate_file))\n",
    "    return\n",
    "\n",
    "def prepare_glove_embeddings():\n",
    "    glove_link = FLAGS['word_embeddings_link']\n",
    "    glove_zip_file = FLAGS['word_embeddings_zipfilename']\n",
    "    glove_text_file = FLAGS['word_embeddings_txtfilename']\n",
    "    \n",
    "    if (not os.path.isfile(glove_zip_file) and not os.path.isfile(glove_text_file)):\n",
    "        print(\"Glove embeddings not found. Downloading from site...\")\n",
    "        import urllib.request\n",
    "        # download glove zip file\n",
    "        urllib.request.urlretrieve(glove_link, glove_zip_file)\n",
    "        print(\"Glove embeddings file downloaded.\")\n",
    "        # extract zip to text file\n",
    "        unzip_single_file(glove_zip_file, glove_text_file)\n",
    "    return\n",
    "\n",
    "def unzip_single_file(zip_file_name, output_file_name):\n",
    "    \"\"\"\n",
    "    If the outfile exists, don't recreate, else create from zipfile\n",
    "    \"\"\"\n",
    "    if not os.path.isfile(output_file_name):\n",
    "        import zipfile\n",
    "        print(\"Unzipping glove embeddings {}..\".format(zip_file_name))\n",
    "        with open(output_file_name, \"wb\") as out_file:\n",
    "            with zipfile.ZipFile(zip_file_name) as zipped:\n",
    "                for info in zipped.infolist():\n",
    "                    if output_file_name in info.filename:\n",
    "                        with zipped.open(info) as requested_file:\n",
    "                            out_file.write(requested_file.read())\n",
    "                            print(\"Glove embeddings unzipped to {}\".format(output_file_name))\n",
    "                            return\n",
    "    return\n",
    "\n",
    "prepare_snli_corpus()\n",
    "prepare_glove_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample word \"the\" with features array([ 4.1800e-01,  2.4968e-01, -4.1242e-01,  1.2170e-01,  3.4527e-01,\n",
      "       -4.4457e-02, -4.9688e-01, -1.7862e-01, -6.6023e-04, -6.5660e-01,\n",
      "        2.7843e-01, -1.4767e-01, -5.5677e-01,  1.4658e-01, -9.5095e-03,\n",
      "        1.1658e-02,  1.0204e-01, -1.2792e-01, -8.4430e-01, -1.2181e-01,\n",
      "       -1.6801e-02, -3.3279e-01, -1.5520e-01, -2.3131e-01, -1.9181e-01,\n",
      "       -1.8823e+00, -7.6746e-01,  9.9051e-02, -4.2125e-01, -1.9526e-01,\n",
      "        4.0071e+00, -1.8594e-01, -5.2287e-01, -3.1681e-01,  5.9213e-04,\n",
      "        7.4449e-03,  1.7778e-01, -1.5897e-01,  1.2041e-02, -5.4223e-02,\n",
      "       -2.9871e-01, -1.5749e-01, -3.4758e-01, -4.5637e-02, -4.4251e-01,\n",
      "        1.8785e-01,  2.7849e-03, -1.8411e-01, -1.1514e-01, -7.8581e-01],\n",
      "      dtype=float32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "387it [00:00, 3866.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Glove wordmap populated, found 400000 vectors\n",
      "\n",
      "Preprocessing snli_1.0_train.txt & parsing to arrays...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "49649it [00:08, 7048.69it/s]\n",
      "  0%|          | 0/50000 [00:00<?, ?it/s]/home/challenger2/anaconda3/envs/venv/lib/python3.6/site-packages/ipykernel_launcher.py:46: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      " 16%|█▋        | 8140/50000 [00:00<00:00, 81392.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Padding premise embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50000/50000 [00:00<00:00, 58440.94it/s]\n",
      " 22%|██▏       | 11182/50000 [00:00<00:00, 111815.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Premise with shape (50000, 30, 50) Padding hypothesis embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50000/50000 [00:00<00:00, 82349.60it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hypothesis with shape (50000, 30, 50).\n",
      "Preprocessing snli_1.0_dev.txt & parsing to arrays...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10000it [00:01, 5107.61it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 66564.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Padding premise embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:00<00:00, 64776.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Premise with shape (10000, 30, 50) Padding hypothesis embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hypothesis with shape (10000, 30, 50).\n"
     ]
    }
   ],
   "source": [
    "def sentence2sequence(sentence):\n",
    "    '''\n",
    "    Turns an input sentence into a (n, d) matrix. \n",
    "    n is the number of tokens in the sentence.\n",
    "    d is the number of dimensions each word vector has.\n",
    "    '''\n",
    "    tokens = None\n",
    "\n",
    "    try:\n",
    "        tokens = sentence.decode().lower().split(\" \")\n",
    "    except AttributeError: # not byte-encoded\n",
    "        tokens = sentence.lower().split(\" \")\n",
    "    rows = []\n",
    "    words = []\n",
    "    for token in tokens: # each token is a word in the sentence\n",
    "        i = len(token)\n",
    "        while len(token) > 0 and i > 0:\n",
    "            word = token[:i]\n",
    "            if word in glove_wordmap:\n",
    "                rows.append(glove_wordmap[word])\n",
    "                words.append(word)\n",
    "                token = token[i:]\n",
    "                i = len(token)\n",
    "            else:\n",
    "                # no such word, add keep reducing until we find a word\n",
    "                i = i - 1\n",
    "    return { \"words\": words, \"rows\": rows }\n",
    "\n",
    "def sentence_score_setup(row):\n",
    "    convert_dict = {\n",
    "        'entailment': 0,\n",
    "        'neutral': 1,\n",
    "        'contradiction': 2\n",
    "    }\n",
    "    score = np.zeros((3,1))\n",
    "    for x in range(1,6):\n",
    "        tag = row[\"label\"+str(x)]\n",
    "        if tag in convert_dict: \n",
    "            score[convert_dict[tag]] += 1\n",
    "    return score / (1.0 * np.sum(score)) # return normalised np array\n",
    "\n",
    "def fit_to_size(matrix, shape):\n",
    "    res = np.zeros(shape)\n",
    "    #print(\"Before: {}\".format(pp.pformat(matrix.shape)))\n",
    "    slices = [slice(0, min(dim, shape[e])) for e, dim in enumerate(matrix.shape)]\n",
    "    res[slices] = matrix[slices]\n",
    "    #print(\"After: {}\".format(pp.pformat(res.shape)))\n",
    "    return res\n",
    "\n",
    "def load_glove_embeddings():\n",
    "    global glove_wordmap\n",
    "    global glove_wordmap_size\n",
    "\n",
    "    glove_text_file = FLAGS['word_embeddings_txtfilename']\n",
    "    printOne = True    \n",
    "\n",
    "    with open(glove_text_file, \"r\", encoding='utf-8') as glove:\n",
    "        for line in glove:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            # tensorflow only accepts arrays, not python lists\n",
    "            featuresMatrix = np.asarray(values[1:], dtype='float32')\n",
    "            # print a sample word with feature matrix\n",
    "            if printOne:\n",
    "                printOne = False\n",
    "                print(\"Sample word \\\"{}\\\" with features {}\".format(word, pp.pformat(featuresMatrix)))\n",
    "            glove_wordmap[word] = featuresMatrix\n",
    "    glove_wordmap_size = len(glove_wordmap)\n",
    "    print(\"Glove wordmap populated, found %s vectors\\n\" % glove_wordmap_size)\n",
    "    \n",
    "def load_snli_data(filename):\n",
    "    if not os.path.isfile(filename):\n",
    "        print(\"ERROR: FILE NOT FOUND. EXITING...\")\n",
    "    else:\n",
    "        print(\"Preprocessing {} & parsing to arrays...\\n\".format(filename))\n",
    "        import csv\n",
    "        with open(filename, \"r\", encoding='utf-8') as data:\n",
    "            train = csv.DictReader(data, delimiter='\\t')\n",
    "            premise_embeds = []\n",
    "            hypothesis_embeds = []\n",
    "            labels = []\n",
    "            scores = []\n",
    "            i = 0\n",
    "            for row in tqdm.tqdm(iterable=train):\n",
    "                i += 1\n",
    "                if i > FLAGS['max_data_items']:\n",
    "                    break\n",
    "                premise_embeds.append(\n",
    "                    np.vstack(sentence2sequence(row[\"sentence1\"].lower())[\"rows\"]))\n",
    "                hypothesis_embeds.append(\n",
    "                    np.vstack(sentence2sequence(row[\"sentence2\"].lower())[\"rows\"]))\n",
    "                labels.append(row[\"gold_label\"])\n",
    "                scores.append(sentence_score_setup(row))\n",
    "                # print(\"Sample data piece: {}\".format(pp.pformat(row)))\n",
    "                # print(pp.pformat(sentence2sequence(row['sentence1'].lower())['rows']))\n",
    "                # print(np.vstack(pp.pformat(sentence2sequence(row['sentence1'].lower())['rows'])))\n",
    "                # print(pp.pformat(sentence_score_setup(row)))\n",
    "            print(\"Padding premise embeddings...\")\n",
    "            max_premise_length = FLAGS['max_premise_length']\n",
    "            max_hypothesis_length = FLAGS['max_hypothesis_length']\n",
    "            embedding_size = FLAGS['embedding_size']\n",
    "            premise_embeds_pad = np.stack([fit_to_size(x, (max_premise_length, embedding_size))\n",
    "                                      for x in tqdm.tqdm(iterable=premise_embeds)])\n",
    "#             premise_embeds_pad = np.array([fit_to_size(x, (max_premise_length, embedding_size))\n",
    "#                                       for x in tqdm.tqdm(iterable=premise_embeds)])\n",
    "            del premise_embeds\n",
    "            print(\"Premise with shape {} Padding hypothesis embeddings...\".format(premise_embeds_pad.shape))\n",
    "            hypothesis_embeds_pad = np.stack([fit_to_size(x, (max_hypothesis_length, embedding_size))\n",
    "                                         for x in tqdm.tqdm(iterable=hypothesis_embeds)])\n",
    "#             hypothesis_embeds_pad = np.array([fit_to_size(x, (max_hypothesis_length, embedding_size))\n",
    "#                                                      for x in tqdm.tqdm(iterable=hypothesis_embeds)])\n",
    "            del hypothesis_embeds\n",
    "            print(\"Hypothesis with shape {}.\".format(hypothesis_embeds_pad.shape))\n",
    "            return (premise_embeds_pad, hypothesis_embeds_pad), labels, np.array(scores)        \n",
    "    \n",
    "load_glove_embeddings()\n",
    "# max_data_items rows processed\n",
    "data_features_tuple, labels, scores = load_snli_data(FLAGS['snli_trainfilename'])\n",
    "v_features_tuple, v_labels, v_scores = load_snli_data(FLAGS['snli_validatefilename'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# a = tf.Variable(tf.ones(shape=(128,30,50)), name=\"a\")\n",
    "# b = tf.Variable(tf.zeros(shape=(128,30,50)), name=\"b\")\n",
    "# c = tf.concat([a, b], 1)\n",
    "# d = tf.transpose(c, [1,0,2])\n",
    "# print(d)\n",
    "# e = tf.reshape(d, [-1, 50])\n",
    "# print(e)\n",
    "# f = tf.split(e, 60)\n",
    "# print(pp.pformat(f))\n",
    "# print(tf.equal(tf.Variable(True), tf.Variable(True)))\n",
    "# print(np.array([[1,2,3], [1,2,3]]).shape[0])\n",
    "# print(data_features_tuple[0].shape)\n",
    "# print(np.random.randint(data_features_tuple[0].shape[0], size=128))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0730 17:10:18.333141 140424002889536 deprecation.py:323] From /home/challenger2/anaconda3/envs/venv/lib/python3.6/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "2019-07-30 17:10:18,333 : WARNING From /home/challenger2/anaconda3/envs/venv/lib/python3.6/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bidirectional (Bidirectional (None, 60, 120)           53280     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 60, 64)            7744      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 60, 3)             195       \n",
      "=================================================================\n",
      "Total params: 61,219\n",
      "Trainable params: 61,219\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "50000\n",
      "50000\n",
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": " logits and labels must have the same first dimension, got logits shape [7680,3] and labels shape [384]\n\t [[node loss/dense_1_loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits (defined at <ipython-input-7-b6650a068cb3>:37) ]] [Op:__inference_keras_scratch_graph_3518]\n\nFunction call stack:\nkeras_scratch_graph\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-b6650a068cb3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_array\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalidate_array\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv_scores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/venv/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    641\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    642\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 643\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    644\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    645\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m~/anaconda3/envs/venv/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\u001b[0m\n\u001b[1;32m    662\u001b[0m         \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    663\u001b[0m         \u001b[0mvalidation_freq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_freq\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 664\u001b[0;31m         steps_name='steps_per_epoch')\n\u001b[0m\u001b[1;32m    665\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    666\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m~/anaconda3/envs/venv/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq, mode, validation_in_fit, prepared_feed_values_from_dataset, steps_name, **kwargs)\u001b[0m\n\u001b[1;32m    381\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    382\u001b[0m         \u001b[0;31m# Get outputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 383\u001b[0;31m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    384\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    385\u001b[0m           \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/venv/lib/python3.6/site-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3508\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3509\u001b[0m       \u001b[0mconverted_inputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3510\u001b[0;31m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mconverted_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3511\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3512\u001b[0m     \u001b[0;31m# EagerTensor.numpy() will often make a copy to ensure memory safety.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/venv/lib/python3.6/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    570\u001b[0m       raise TypeError(\"Keyword arguments {} unknown. Expected {}.\".format(\n\u001b[1;32m    571\u001b[0m           list(kwargs.keys()), list(self._arg_keywords)))\n\u001b[0;32m--> 572\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    573\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/venv/lib/python3.6/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    669\u001b[0m     \u001b[0;31m# Only need to override the gradient in graph mode and when we have outputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    670\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 671\u001b[0;31m       \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inference_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    672\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    673\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_register_gradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/venv/lib/python3.6/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args)\u001b[0m\n\u001b[1;32m    443\u001b[0m             attrs=(\"executor_type\", executor_type,\n\u001b[1;32m    444\u001b[0m                    \"config_proto\", config),\n\u001b[0;32m--> 445\u001b[0;31m             ctx=ctx)\n\u001b[0m\u001b[1;32m    446\u001b[0m       \u001b[0;31m# Replace empty list with None\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    447\u001b[0m       \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/venv/lib/python3.6/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m     \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_keras_symbolic_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/venv/lib/python3.6/site-packages/six.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m:  logits and labels must have the same first dimension, got logits shape [7680,3] and labels shape [384]\n\t [[node loss/dense_1_loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits (defined at <ipython-input-7-b6650a068cb3>:37) ]] [Op:__inference_keras_scratch_graph_3518]\n\nFunction call stack:\nkeras_scratch_graph\n"
     ]
    }
   ],
   "source": [
    "input_length = FLAGS['max_premise_length'] + FLAGS['max_hypothesis_length'] # first layer length\n",
    "batch_size = FLAGS['batch_size']\n",
    "premise_len = FLAGS['max_premise_length']\n",
    "hypothesis_len = FLAGS['max_hypothesis_length']\n",
    "dim = FLAGS['embedding_size']\n",
    "hidden_len = FLAGS['hidden_length']\n",
    "num_epochs = FLAGS['num_epochs']\n",
    "max_data_items = FLAGS['max_data_items']\n",
    "\n",
    "LABELS = {\n",
    "    'entailment': 0,\n",
    "    'neutral': 1,\n",
    "    'contradiction': 2\n",
    "}\n",
    "# print(data_features_tuple[0].shape)\n",
    "# premise_embeds_tensor = tf.Variable(data_features_tuple[0]) \n",
    "# hypothesis_embeds_tensor = tf.Variable(data_features_tuple[1])\n",
    "# print(premise_embeds_tensor.shape)\n",
    "# x = tf.keras.layers.concatenate([premise_embeds_tensor, hypothesis_embeds_tensor], axis=1)\n",
    "# print(x.shape)\n",
    "\n",
    "# (max_inputs, 60, 50)\n",
    "# input_tensor = layers.concatenate([data_features_tuple[0], data_features_tuple[1]], axis=1)\n",
    "input_array = np.concatenate((data_features_tuple[0], data_features_tuple[1]), axis=1)\n",
    "# validate_tensor = layers.concatenate([v_features_tuple[0], v_features_tuple[1]], axis=1)\n",
    "validate_array = np.concatenate((v_features_tuple[0], v_features_tuple[1]), axis=1)\n",
    "\n",
    "model = keras.Sequential()\n",
    "model.add(layers.Bidirectional(layers.LSTM(input_length, return_sequences=True), input_shape=(60, dim)))\n",
    "model.add(layers.Dense(hidden_len, activation=\"relu\"))\n",
    "model.add(layers.Dense(len(LABELS), activation=\"softmax\"))\n",
    "model.compile(optimizer=\"rmsprop\", loss=\"sparse_categorical_crossentropy\", metrics=['accuracy'])\n",
    "model.summary()\n",
    "print(len(input_array))\n",
    "print(len(scores))\n",
    "\n",
    "model.fit(x=input_array, y=scores, batch_size=128, epochs=num_epochs, validation_data=(validate_array, v_scores), verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

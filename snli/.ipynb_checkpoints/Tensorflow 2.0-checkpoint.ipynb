{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load Hyperparameters and global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{ 'batch_size': 128,\n",
      "  'buffer_size': 10000,\n",
      "  'config_file': 'snli.config',\n",
      "  'embedding_size': 50,\n",
      "  'hidden_length': 64,\n",
      "  'max_data_items': 50000,\n",
      "  'max_features': 50000,\n",
      "  'max_hypothesis_length': 30,\n",
      "  'max_premise_length': 30,\n",
      "  'num_epochs': 5,\n",
      "  'snli_link': 'https://nlp.stanford.edu/projects/snli/snli_1.0.zip',\n",
      "  'snli_testfilename': 'snli_1.0_test.txt',\n",
      "  'snli_trainfilename': 'snli_1.0_train.txt',\n",
      "  'snli_validatefilename': 'snli_1.0_validate.txt',\n",
      "  'snli_zipfilename': 'snli_1.0.zip',\n",
      "  'word_embeddings_link': 'http://nlp.stanford.edu/data/glove.6B.zip',\n",
      "  'word_embeddings_txtfilename': 'glove.6B.50d.txt',\n",
      "  'word_embeddings_zipfilename': 'glove.6B.zip'}\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import argparse\n",
    "import tqdm\n",
    "import logging\n",
    "import pprint # pretty print python objects\n",
    "import sys\n",
    "import os\n",
    "\n",
    "FLAGS = {\n",
    "    \"config_file\": \"snli.config\",\n",
    "    \"buffer_size\": 10000,\n",
    "    \"max_data_items\": 50000,\n",
    "    \"snli_link\": \"https://nlp.stanford.edu/projects/snli/snli_1.0.zip\",\n",
    "    \"snli_zipfilename\": \"snli_1.0.zip\",\n",
    "    \"snli_trainfilename\": \"snli_1.0_train.txt\",\n",
    "    \"snli_validatefilename\": \"snli_1.0_validate.txt\",\n",
    "    \"snli_testfilename\": \"snli_1.0_test.txt\",\n",
    "    \"word_embeddings_link\": \"http://nlp.stanford.edu/data/glove.6B.zip\",\n",
    "    \"word_embeddings_zipfilename\": \"glove.6B.zip\",\n",
    "    \"word_embeddings_txtfilename\": \"glove.6B.50d.txt\",\n",
    "    \"max_premise_length\": 30,\n",
    "    \"max_hypothesis_length\": 30,\n",
    "    \"batch_size\": 128,\n",
    "    \"hidden_length\": 64,\n",
    "    \"embedding_size\": 50, # 50 dim embeddings\n",
    "    \"max_features\": 50000,\n",
    "    \"num_epochs\": 5\n",
    "}\n",
    "\n",
    "def create_logger():\n",
    "    log = logging.getLogger() # root logger\n",
    "    log.setLevel(logging.DEBUG)\n",
    "    formatter = logging.Formatter(fmt=\"%(asctime)s : %(levelname)s %(message)s\")\n",
    "    handler = logging.StreamHandler()\n",
    "    handler.setFormatter(formatter)\n",
    "    log.addHandler(handler)\n",
    "    return logging.getLogger()\n",
    "\n",
    "pp = pprint.PrettyPrinter(indent=2)\n",
    "logger = create_logger()\n",
    "glove_wordmap = {}\n",
    "glove_wordmap_size = 0\n",
    "\n",
    "print(pp.pformat(FLAGS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Snli corpus file already downloaded. Extracting...\n",
      "snli_1.0_train.txt already extracted.\n",
      "\n",
      "snli_1.0_validate.txt already extracted.\n",
      "\n",
      "snli_1.0_validate.txt already extracted.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load dataset manually\n",
    "def prepare_snli_corpus():\n",
    "    snli_link = FLAGS['snli_link']\n",
    "    snli_zip_file = FLAGS['snli_zipfilename']\n",
    "    snli_train_file = FLAGS['snli_trainfilename']\n",
    "    snli_validate_file = FLAGS['snli_validatefilename']\n",
    "    snli_test_file = FLAGS['snli_testfilename']\n",
    "    \n",
    "    if (not os.path.isfile(snli_zip_file)):\n",
    "        print(\"Snli corpus not found. Downloading from site...\")\n",
    "        import urllib.request\n",
    "        # download glove zip file\n",
    "        urllib.request.urlretrieve(snli_link, snli_zip_file)\n",
    "    print(\"Snli corpus file already downloaded. Extracting...\")\n",
    "    # extract train, validate and test files\n",
    "    if (not os.path.isfile(snli_train_file)):\n",
    "        unzip_single_file(snli_zip_file, snli_train_file)\n",
    "        print(\"Extracted {}\\n\".format(snli_validate_file))\n",
    "    else:\n",
    "        print(\"{} already extracted.\\n\".format(snli_train_file))\n",
    "    if (not os.path.isfile(snli_validate_file)):\n",
    "        unzip_single_file(snli_zip_file, snli_validate_file)\n",
    "        print(\"Extracted {}\\n\".format(snli_validate_file))\n",
    "    else:\n",
    "        print(\"{} already extracted.\\n\".format(snli_validate_file))\n",
    "    if (not os.path.isfile(snli_test_file)):\n",
    "        unzip_single_file(snli_zip_file, snli_test_file)\n",
    "        print(\"Extracted {}\\n\".format(snli_test_file))\n",
    "    else:\n",
    "        print(\"{} already extracted.\\n\".format(snli_validate_file))\n",
    "    return\n",
    "\n",
    "def prepare_glove_embeddings():\n",
    "    glove_link = FLAGS['word_embeddings_link']\n",
    "    glove_zip_file = FLAGS['word_embeddings_zipfilename']\n",
    "    glove_text_file = FLAGS['word_embeddings_txtfilename']\n",
    "    \n",
    "    if (not os.path.isfile(glove_zip_file) and not os.path.isfile(glove_text_file)):\n",
    "        print(\"Glove embeddings not found. Downloading from site...\")\n",
    "        import urllib.request\n",
    "        # download glove zip file\n",
    "        urllib.request.urlretrieve(glove_link, glove_zip_file)\n",
    "        print(\"Glove embeddings file downloaded.\")\n",
    "        # extract zip to text file\n",
    "        unzip_single_file(glove_zip_file, glove_text_file)\n",
    "    return\n",
    "\n",
    "def unzip_single_file(zip_file_name, output_file_name):\n",
    "    \"\"\"\n",
    "    If the outfile exists, don't recreate, else create from zipfile\n",
    "    \"\"\"\n",
    "    if not os.path.isfile(output_file_name):\n",
    "        import zipfile\n",
    "        print(\"Unzipping glove embeddings {}..\".format(zip_file_name))\n",
    "        with open(output_file_name, \"wb\") as out_file:\n",
    "            with zipfile.ZipFile(zip_file_name) as zipped:\n",
    "                for info in zipped.infolist():\n",
    "                    if output_file_name in info.filename:\n",
    "                        with zipped.open(info) as requested_file:\n",
    "                            out_file.write(requested_file.read())\n",
    "                            print(\"Glove embeddings unzipped to {}\".format(output_file_name))\n",
    "                            return\n",
    "    return\n",
    "\n",
    "prepare_snli_corpus()\n",
    "prepare_glove_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample word \"the\" with features array([ 4.1800e-01,  2.4968e-01, -4.1242e-01,  1.2170e-01,  3.4527e-01,\n",
      "       -4.4457e-02, -4.9688e-01, -1.7862e-01, -6.6023e-04, -6.5660e-01,\n",
      "        2.7843e-01, -1.4767e-01, -5.5677e-01,  1.4658e-01, -9.5095e-03,\n",
      "        1.1658e-02,  1.0204e-01, -1.2792e-01, -8.4430e-01, -1.2181e-01,\n",
      "       -1.6801e-02, -3.3279e-01, -1.5520e-01, -2.3131e-01, -1.9181e-01,\n",
      "       -1.8823e+00, -7.6746e-01,  9.9051e-02, -4.2125e-01, -1.9526e-01,\n",
      "        4.0071e+00, -1.8594e-01, -5.2287e-01, -3.1681e-01,  5.9213e-04,\n",
      "        7.4449e-03,  1.7778e-01, -1.5897e-01,  1.2041e-02, -5.4223e-02,\n",
      "       -2.9871e-01, -1.5749e-01, -3.4758e-01, -4.5637e-02, -4.4251e-01,\n",
      "        1.8785e-01,  2.7849e-03, -1.8411e-01, -1.1514e-01, -7.8581e-01],\n",
      "      dtype=float32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "775it [00:00, 7748.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Glove wordmap populated, found 400000 vectors\n",
      "\n",
      "Preprocessing snli data & parsing to arrays...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "49163it [00:06, 8769.86it/s]\n",
      "  0%|          | 0/50000 [00:00<?, ?it/s]/home/challenger2/anaconda3/envs/venv/lib/python3.6/site-packages/ipykernel_launcher.py:46: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      " 17%|█▋        | 8318/50000 [00:00<00:00, 83176.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Padding premise embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50000/50000 [00:00<00:00, 84955.57it/s]\n",
      " 24%|██▍       | 12125/50000 [00:00<00:00, 121244.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Premise with shape (50000, 30, 50) Padding hypothesis embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50000/50000 [00:00<00:00, 97679.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hypothesis with shape (50000, 30, 50).\n"
     ]
    }
   ],
   "source": [
    "def sentence2sequence(sentence):\n",
    "    '''\n",
    "    Turns an input sentence into a (n, d) matrix. \n",
    "    n is the number of tokens in the sentence.\n",
    "    d is the number of dimensions each word vector has.\n",
    "    '''\n",
    "    tokens = None\n",
    "\n",
    "    try:\n",
    "        tokens = sentence.decode().lower().split(\" \")\n",
    "    except AttributeError: # not byte-encoded\n",
    "        tokens = sentence.lower().split(\" \")\n",
    "    rows = []\n",
    "    words = []\n",
    "    for token in tokens: # each token is a word in the sentence\n",
    "        i = len(token)\n",
    "        while len(token) > 0 and i > 0:\n",
    "            word = token[:i]\n",
    "            if word in glove_wordmap:\n",
    "                rows.append(glove_wordmap[word])\n",
    "                words.append(word)\n",
    "                token = token[i:]\n",
    "                i = len(token)\n",
    "            else:\n",
    "                # no such word, add keep reducing until we find a word\n",
    "                i = i - 1\n",
    "    return { \"words\": words, \"rows\": rows }\n",
    "\n",
    "def sentence_score_setup(row):\n",
    "    convert_dict = {\n",
    "        'entailment': 0,\n",
    "        'neutral': 1,\n",
    "        'contradiction': 2\n",
    "    }\n",
    "    score = np.zeros((3,1))\n",
    "    for x in range(1,6):\n",
    "        tag = row[\"label\"+str(x)]\n",
    "        if tag in convert_dict: \n",
    "            score[convert_dict[tag]] += 1\n",
    "    return score / (1.0 * np.sum(score)) # return normalised np array\n",
    "\n",
    "def fit_to_size(matrix, shape):\n",
    "    res = np.zeros(shape)\n",
    "    #print(\"Before: {}\".format(pp.pformat(matrix.shape)))\n",
    "    slices = [slice(0, min(dim, shape[e])) for e, dim in enumerate(matrix.shape)]\n",
    "    res[slices] = matrix[slices]\n",
    "    #print(\"After: {}\".format(pp.pformat(res.shape)))\n",
    "    return res\n",
    "\n",
    "def load_glove_embeddings():\n",
    "    global glove_wordmap\n",
    "    global glove_wordmap_size\n",
    "\n",
    "    glove_text_file = FLAGS['word_embeddings_txtfilename']\n",
    "    printOne = True    \n",
    "\n",
    "    with open(glove_text_file, \"r\", encoding='utf-8') as glove:\n",
    "        for line in glove:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            # tensorflow only accepts arrays, not python lists\n",
    "            featuresMatrix = np.asarray(values[1:], dtype='float32')\n",
    "            # print a sample word with feature matrix\n",
    "            if printOne:\n",
    "                printOne = False\n",
    "                print(\"Sample word \\\"{}\\\" with features {}\".format(word, pp.pformat(featuresMatrix)))\n",
    "            glove_wordmap[word] = featuresMatrix\n",
    "    glove_wordmap_size = len(glove_wordmap)\n",
    "    print(\"Glove wordmap populated, found %s vectors\\n\" % glove_wordmap_size)\n",
    "    \n",
    "def load_snli_data(filename):\n",
    "    if not os.path.isfile(filename):\n",
    "        print(\"ERROR: FILE NOT FOUND. EXITING...\")\n",
    "    else:\n",
    "        print(\"Preprocessing snli data & parsing to arrays...\")\n",
    "        import csv\n",
    "        with open(filename, \"r\", encoding='utf-8') as data:\n",
    "            train = csv.DictReader(data, delimiter='\\t')\n",
    "            premise_embeds = []\n",
    "            hypothesis_embeds = []\n",
    "            labels = []\n",
    "            scores = []\n",
    "            i = 0\n",
    "            for row in tqdm.tqdm(iterable=train):\n",
    "                i += 1\n",
    "                if i > FLAGS['max_data_items']:\n",
    "                    break\n",
    "                premise_embeds.append(\n",
    "                    np.vstack(sentence2sequence(row[\"sentence1\"].lower())[\"rows\"]))\n",
    "                hypothesis_embeds.append(\n",
    "                    np.vstack(sentence2sequence(row[\"sentence2\"].lower())[\"rows\"]))\n",
    "                labels.append(row[\"gold_label\"])\n",
    "                scores.append(sentence_score_setup(row))\n",
    "                # print(\"Sample data piece: {}\".format(pp.pformat(row)))\n",
    "                # print(pp.pformat(sentence2sequence(row['sentence1'].lower())['rows']))\n",
    "                # print(np.vstack(pp.pformat(sentence2sequence(row['sentence1'].lower())['rows'])))\n",
    "                # print(pp.pformat(sentence_score_setup(row)))\n",
    "            print(\"Padding premise embeddings...\")\n",
    "            max_premise_length = FLAGS['max_premise_length']\n",
    "            max_hypothesis_length = FLAGS['max_hypothesis_length']\n",
    "            embedding_size = FLAGS['embedding_size']\n",
    "#             premise_embeds_pad = np.vstack([fit_to_size(x, (max_premise_length, embedding_size))\n",
    "#                                       for x in tqdm.tqdm(iterable=premise_embeds)])\n",
    "            premise_embeds_pad = np.array([fit_to_size(x, (max_premise_length, embedding_size))\n",
    "                                      for x in tqdm.tqdm(iterable=premise_embeds)])\n",
    "            del premise_embeds\n",
    "            print(\"Premise with shape {} Padding hypothesis embeddings...\".format(premise_embeds_pad.shape))\n",
    "#             hypothesis_embeds_pad = np.vstack([fit_to_size(x, (max_hypothesis_length, embedding_size))\n",
    "#                                          for x in tqdm.tqdm(iterable=hypothesis_embeds)])\n",
    "            hypothesis_embeds_pad = np.array([fit_to_size(x, (max_hypothesis_length, embedding_size))\n",
    "                                                     for x in tqdm.tqdm(iterable=hypothesis_embeds)])\n",
    "            del hypothesis_embeds\n",
    "            print(\"Hypothesis with shape {}.\".format(hypothesis_embeds_pad.shape))\n",
    "            return (premise_embeds_pad, hypothesis_embeds_pad), labels, scores        \n",
    "    \n",
    "load_glove_embeddings()\n",
    "# max_data_items rows processed\n",
    "data_features_tuple, labels, scores = load_snli_data(FLAGS['snli_trainfilename'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = tf.Variable(tf.ones(shape=(128,30,50)), name=\"a\")\n",
    "b = tf.Variable(tf.zeros(shape=(128,30,50)), name=\"b\")\n",
    "c = tf.concat([a, b], 1)\n",
    "d = tf.transpose(c, [1,0,2])\n",
    "# print(d)\n",
    "e = tf.reshape(d, [-1, 50])\n",
    "# print(e)\n",
    "f = tf.split(e, 60)\n",
    "#print(pp.pformat(f))\n",
    "# print(tf.equal(tf.Variable(True), tf.Variable(True)))\n",
    "# print(np.array([[1,2,3], [1,2,3]]).shape[0])\n",
    "#print(data_features_tuple[0].shape)\n",
    "#print(np.random.randint(data_features_tuple[0].shape[0], size=128))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "('Keyword argument not understood:', 'input_length')",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-631b8f1a0e3c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0minput_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdata_features_tuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_features_tuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSequential\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBidirectional\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLSTM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_sequences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBidirectional\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLSTM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/venv/lib/python3.6/site-packages/tensorflow/python/keras/layers/wrappers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, layer, merge_mode, weights, backward_layer, **kwargs)\u001b[0m\n\u001b[1;32m    457\u001b[0m     \u001b[0;31m# of it we actually run.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    458\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 459\u001b[0;31m     \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBidirectional\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    460\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    461\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_spec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_spec\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/venv/lib/python3.6/site-packages/tensorflow/python/keras/layers/wrappers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, layer, **kwargs)\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0;31m# to the inputs to the Wrapper layer).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_input_map\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m     \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mWrapper\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/venv/lib/python3.6/site-packages/tensorflow/python/training/tracking/base.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    457\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 458\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    459\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    460\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprevious_value\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/venv/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, trainable, name, dtype, dynamic, **kwargs)\u001b[0m\n\u001b[1;32m    159\u001b[0m     }\n\u001b[1;32m    160\u001b[0m     \u001b[0;31m# Validate optional keyword arguments.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 161\u001b[0;31m     \u001b[0mgeneric_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidate_kwargs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallowed_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m     \u001b[0;31m# Mutable properties\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/venv/lib/python3.6/site-packages/tensorflow/python/keras/utils/generic_utils.py\u001b[0m in \u001b[0;36mvalidate_kwargs\u001b[0;34m(kwargs, allowed_kwargs, error_message)\u001b[0m\n\u001b[1;32m    598\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mkwarg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    599\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mkwarg\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mallowed_kwargs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 600\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_message\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwarg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: ('Keyword argument not understood:', 'input_length')"
     ]
    }
   ],
   "source": [
    "input_len = FLAGS['max_premise_length'] + FLAGS['max_hypothesis_length'] # first layer length\n",
    "batch_size = FLAGS['batch_size']\n",
    "premise_len = FLAGS['max_premise_length']\n",
    "hypothesis_len = FLAGS['max_hypothesis_length']\n",
    "dim = FLAGS['embedding_size']\n",
    "hidden_len = FLAGS['hidden_length']\n",
    "\n",
    "# print(data_features_tuple[0].shape)\n",
    "# premise_embeds_tensor = tf.Variable(data_features_tuple[0]) \n",
    "# hypothesis_embeds_tensor = tf.Variable(data_features_tuple[1])\n",
    "# print(premise_embeds_tensor.shape)\n",
    "# x = tf.keras.layers.concatenate([premise_embeds_tensor, hypothesis_embeds_tensor], axis=1)\n",
    "# print(x.shape)\n",
    "\n",
    "# (max_inputs, 60, 50)\n",
    "input_tensor = layers.concatenate([data_features_tuple[0], data_features_tuple[1]], axis=1)\n",
    "model = keras.Sequential()\n",
    "model.add(layers.Bidirectional(layers.LSTM(input_length, return_sequences=True), input_shape=(input_len, dim)))\n",
    "model.add(layers.Bidirectional(layers.LSTM(input_length)))\n",
    "model.add(layers.Dense(hidden_length))\n",
    "model.add(layers.Activation(\"relu\"))\n",
    "model.compile(optimizer=\"rmsprop\", loss=\"categorical_crossentropy\")\n",
    "mode.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

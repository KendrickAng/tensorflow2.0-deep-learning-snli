{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Hyperparameters and global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{ 'batch_size': 128,\n",
      "  'buffer_size': 10000,\n",
      "  'config_file': 'snli.config',\n",
      "  'embedding_size': 50,\n",
      "  'hidden_length': 64,\n",
      "  'max_features': 50000,\n",
      "  'max_hypothesis_length': 30,\n",
      "  'max_premise_length': 30,\n",
      "  'num_epochs': 5,\n",
      "  'snli_link': 'https://nlp.stanford.edu/projects/snli/snli_1.0.zip',\n",
      "  'snli_testfilename': 'snli_1.0_test.txt',\n",
      "  'snli_trainfilename': 'snli_1.0_train.txt',\n",
      "  'snli_validatefilename': 'snli_1.0_validate.txt',\n",
      "  'snli_zipfilename': 'snli_1.0.zip',\n",
      "  'word_embeddings_link': 'http://nlp.stanford.edu/data/glove.6B.zip',\n",
      "  'word_embeddings_txtfilename': 'glove.6B.50d.txt',\n",
      "  'word_embeddings_zipfilename': 'glove.6B.zip'}\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import argparse\n",
    "import tqdm\n",
    "import logging\n",
    "import pprint # pretty print python objects\n",
    "import sys\n",
    "import os\n",
    "\n",
    "FLAGS = {\n",
    "    \"config_file\": \"snli.config\",\n",
    "    \"buffer_size\": 10000,\n",
    "    \"snli_link\": \"https://nlp.stanford.edu/projects/snli/snli_1.0.zip\",\n",
    "    \"snli_zipfilename\": \"snli_1.0.zip\",\n",
    "    \"snli_trainfilename\": \"snli_1.0_train.txt\",\n",
    "    \"snli_validatefilename\": \"snli_1.0_validate.txt\",\n",
    "    \"snli_testfilename\": \"snli_1.0_test.txt\",\n",
    "    \"word_embeddings_link\": \"http://nlp.stanford.edu/data/glove.6B.zip\",\n",
    "    \"word_embeddings_zipfilename\": \"glove.6B.zip\",\n",
    "    \"word_embeddings_txtfilename\": \"glove.6B.50d.txt\",\n",
    "    \"max_premise_length\": 30,\n",
    "    \"max_hypothesis_length\": 30,\n",
    "    \"batch_size\": 128,\n",
    "    \"hidden_length\": 64,\n",
    "    \"embedding_size\": 50, # 50 dim embeddings\n",
    "    \"max_features\": 50000,\n",
    "    \"num_epochs\": 5\n",
    "}\n",
    "\n",
    "def create_logger():\n",
    "    log = logging.getLogger() # root logger\n",
    "    log.setLevel(logging.DEBUG)\n",
    "    formatter = logging.Formatter(fmt=\"%(asctime)s : %(levelname)s %(message)s\")\n",
    "    handler = logging.StreamHandler()\n",
    "    handler.setFormatter(formatter)\n",
    "    log.addHandler(handler)\n",
    "    return logging.getLogger()\n",
    "\n",
    "pp = pprint.PrettyPrinter(indent=2)\n",
    "logger = create_logger()\n",
    "glove_wordmap = {}\n",
    "glove_wordmap_size = 0\n",
    "\n",
    "print(pp.pformat(FLAGS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Snli corpus file already downloaded. Extracting...\n",
      "snli_1.0_train.txt already extracted.\n",
      "\n",
      "snli_1.0_validate.txt already extracted.\n",
      "\n",
      "snli_1.0_validate.txt already extracted.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load dataset manually\n",
    "def prepare_snli_corpus():\n",
    "    snli_link = FLAGS['snli_link']\n",
    "    snli_zip_file = FLAGS['snli_zipfilename']\n",
    "    snli_train_file = FLAGS['snli_trainfilename']\n",
    "    snli_validate_file = FLAGS['snli_validatefilename']\n",
    "    snli_test_file = FLAGS['snli_testfilename']\n",
    "    \n",
    "    if (not os.path.isfile(snli_zip_file)):\n",
    "        print(\"Snli corpus not found. Downloading from site...\")\n",
    "        import urllib.request\n",
    "        # download glove zip file\n",
    "        urllib.request.urlretrieve(snli_link, snli_zip_file)\n",
    "    print(\"Snli corpus file already downloaded. Extracting...\")\n",
    "    # extract train, validate and test files\n",
    "    if (not os.path.isfile(snli_train_file)):\n",
    "        unzip_single_file(snli_zip_file, snli_train_file)\n",
    "        print(\"Extracted {}\\n\".format(snli_validate_file))\n",
    "    else:\n",
    "        print(\"{} already extracted.\\n\".format(snli_train_file))\n",
    "    if (not os.path.isfile(snli_validate_file)):\n",
    "        unzip_single_file(snli_zip_file, snli_validate_file)\n",
    "        print(\"Extracted {}\\n\".format(snli_validate_file))\n",
    "    else:\n",
    "        print(\"{} already extracted.\\n\".format(snli_validate_file))\n",
    "    if (not os.path.isfile(snli_test_file)):\n",
    "        unzip_single_file(snli_zip_file, snli_test_file)\n",
    "        print(\"Extracted {}\\n\".format(snli_test_file))\n",
    "    else:\n",
    "        print(\"{} already extracted.\\n\".format(snli_validate_file))\n",
    "    return\n",
    "\n",
    "def prepare_glove_embeddings():\n",
    "    glove_link = FLAGS['word_embeddings_link']\n",
    "    glove_zip_file = FLAGS['word_embeddings_zipfilename']\n",
    "    glove_text_file = FLAGS['word_embeddings_txtfilename']\n",
    "    \n",
    "    if (not os.path.isfile(glove_zip_file) and not os.path.isfile(glove_text_file)):\n",
    "        print(\"Glove embeddings not found. Downloading from site...\")\n",
    "        import urllib.request\n",
    "        # download glove zip file\n",
    "        urllib.request.urlretrieve(glove_link, glove_zip_file)\n",
    "        print(\"Glove embeddings file downloaded.\")\n",
    "        # extract zip to text file\n",
    "        unzip_single_file(glove_zip_file, glove_text_file)\n",
    "    return\n",
    "\n",
    "def unzip_single_file(zip_file_name, output_file_name):\n",
    "    \"\"\"\n",
    "    If the outfile exists, don't recreate, else create from zipfile\n",
    "    \"\"\"\n",
    "    if not os.path.isfile(output_file_name):\n",
    "        import zipfile\n",
    "        print(\"Unzipping glove embeddings {}..\".format(zip_file_name))\n",
    "        with open(output_file_name, \"wb\") as out_file:\n",
    "            with zipfile.ZipFile(zip_file_name) as zipped:\n",
    "                for info in zipped.infolist():\n",
    "                    if output_file_name in info.filename:\n",
    "                        with zipped.open(info) as requested_file:\n",
    "                            out_file.write(requested_file.read())\n",
    "                            print(\"Glove embeddings unzipped to {}\".format(output_file_name))\n",
    "                            return\n",
    "    return\n",
    "\n",
    "prepare_snli_corpus()\n",
    "prepare_glove_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample word \"the\" with features array([ 4.1800e-01,  2.4968e-01, -4.1242e-01,  1.2170e-01,  3.4527e-01,\n",
      "       -4.4457e-02, -4.9688e-01, -1.7862e-01, -6.6023e-04, -6.5660e-01,\n",
      "        2.7843e-01, -1.4767e-01, -5.5677e-01,  1.4658e-01, -9.5095e-03,\n",
      "        1.1658e-02,  1.0204e-01, -1.2792e-01, -8.4430e-01, -1.2181e-01,\n",
      "       -1.6801e-02, -3.3279e-01, -1.5520e-01, -2.3131e-01, -1.9181e-01,\n",
      "       -1.8823e+00, -7.6746e-01,  9.9051e-02, -4.2125e-01, -1.9526e-01,\n",
      "        4.0071e+00, -1.8594e-01, -5.2287e-01, -3.1681e-01,  5.9213e-04,\n",
      "        7.4449e-03,  1.7778e-01, -1.5897e-01,  1.2041e-02, -5.4223e-02,\n",
      "       -2.9871e-01, -1.5749e-01, -3.4758e-01, -4.5637e-02, -4.4251e-01,\n",
      "        1.8785e-01,  2.7849e-03, -1.8411e-01, -1.1514e-01, -7.8581e-01],\n",
      "      dtype=float32)\n",
      "Glove wordmap populated, found 400000 vectors\n",
      "\n",
      "Preprocessing snli data & parsing to arrays...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample data piece: OrderedDict([ ('gold_label', 'neutral'),\n",
      "              ( 'sentence1_binary_parse',\n",
      "                '( ( ( A person ) ( on ( a horse ) ) ) ( ( jumps ( over ( a ( '\n",
      "                'broken ( down airplane ) ) ) ) ) . ) )'),\n",
      "              ( 'sentence2_binary_parse',\n",
      "                '( ( A person ) ( ( is ( ( training ( his horse ) ) ( for ( a '\n",
      "                'competition ) ) ) ) . ) )'),\n",
      "              ( 'sentence1_parse',\n",
      "                '(ROOT (S (NP (NP (DT A) (NN person)) (PP (IN on) (NP (DT a) '\n",
      "                '(NN horse)))) (VP (VBZ jumps) (PP (IN over) (NP (DT a) (JJ '\n",
      "                'broken) (JJ down) (NN airplane)))) (. .)))'),\n",
      "              ( 'sentence2_parse',\n",
      "                '(ROOT (S (NP (DT A) (NN person)) (VP (VBZ is) (VP (VBG '\n",
      "                'training) (NP (PRP$ his) (NN horse)) (PP (IN for) (NP (DT a) '\n",
      "                '(NN competition))))) (. .)))'),\n",
      "              ( 'sentence1',\n",
      "                'A person on a horse jumps over a broken down airplane.'),\n",
      "              ( 'sentence2',\n",
      "                'A person is training his horse for a competition.'),\n",
      "              ('captionID', '3416050480.jpg#4'),\n",
      "              ('pairID', '3416050480.jpg#4r1n'),\n",
      "              ('label1', 'neutral'),\n",
      "              ('label2', ''),\n",
      "              ('label3', ''),\n",
      "              ('label4', ''),\n",
      "              ('label5', '')])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before: (12, 50)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:46: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After: (30, 50)\n",
      "Before: (10, 50)\n",
      "After: (30, 50)\n"
     ]
    }
   ],
   "source": [
    "def sentence2sequence(sentence):\n",
    "    '''\n",
    "    Turns an input sentence into a (n, d) matrix. \n",
    "    n is the number of tokens in the sentence.\n",
    "    d is the number of dimensions each word vector has.\n",
    "    '''\n",
    "    tokens = None\n",
    "\n",
    "    try:\n",
    "        tokens = sentence.decode().lower().split(\" \")\n",
    "    except AttributeError: # not byte-encoded\n",
    "        tokens = sentence.lower().split(\" \")\n",
    "    rows = []\n",
    "    words = []\n",
    "    for token in tokens: # each token is a word in the sentence\n",
    "        i = len(token)\n",
    "        while len(token) > 0 and i > 0:\n",
    "            word = token[:i]\n",
    "            if word in glove_wordmap:\n",
    "                rows.append(glove_wordmap[word])\n",
    "                words.append(word)\n",
    "                token = token[i:]\n",
    "                i = len(token)\n",
    "            else:\n",
    "                # no such word, add keep reducing until we find a word\n",
    "                i = i - 1\n",
    "    return { \"words\": words, \"rows\": rows }\n",
    "\n",
    "def sentence_score_setup(row):\n",
    "    convert_dict = {\n",
    "        'entailment': 0,\n",
    "        'neutral': 1,\n",
    "        'contradiction': 2\n",
    "    }\n",
    "    score = np.zeros((3,1))\n",
    "    for x in range(1,6):\n",
    "        tag = row[\"label\"+str(x)]\n",
    "        if tag in convert_dict: \n",
    "            score[convert_dict[tag]] += 1\n",
    "    return score / (1.0 * np.sum(score)) # return normalised np array\n",
    "\n",
    "def fit_to_size(matrix, shape):\n",
    "    res = np.zeros(shape)\n",
    "    #print(\"Before: {}\".format(pp.pformat(matrix.shape)))\n",
    "    slices = [slice(0, min(dim, shape[e])) for e, dim in enumerate(matrix.shape)]\n",
    "    res[slices] = matrix[slices]\n",
    "    #print(\"After: {}\".format(pp.pformat(res.shape)))\n",
    "    return res\n",
    "\n",
    "def load_glove_embeddings():\n",
    "    global glove_wordmap\n",
    "    global glove_wordmap_size\n",
    "\n",
    "    glove_text_file = FLAGS['word_embeddings_txtfilename']\n",
    "    printOne = True    \n",
    "\n",
    "    with open(glove_text_file, \"r\", encoding='utf-8') as glove:\n",
    "        for line in glove:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            # tensorflow only accepts arrays, not python lists\n",
    "            featuresMatrix = np.asarray(values[1:], dtype='float32')\n",
    "            # print a sample word with feature matrix\n",
    "            if printOne:\n",
    "                printOne = False\n",
    "                print(\"Sample word \\\"{}\\\" with features {}\".format(word, pp.pformat(featuresMatrix)))\n",
    "            glove_wordmap[word] = featuresMatrix\n",
    "    glove_wordmap_size = len(glove_wordmap)\n",
    "    print(\"Glove wordmap populated, found %s vectors\\n\" % glove_wordmap_size)\n",
    "    \n",
    "def load_snli_data(filename):\n",
    "    if not os.path.isfile(filename):\n",
    "        print(\"ERROR: FILE NOT FOUND. EXITING...\")\n",
    "    else:\n",
    "        print(\"Preprocessing snli data & parsing to arrays...\")\n",
    "        import csv\n",
    "        with open(filename, \"r\", encoding='utf-8') as data:\n",
    "            train = csv.DictReader(data, delimiter='\\t')\n",
    "            premise_embeds = []\n",
    "            hypothesis_embeds = []\n",
    "            labels = []\n",
    "            scores = []\n",
    "            for row in tqdm.tqdm(iterable=train):\n",
    "                premise_embeds.append(\n",
    "                    np.vstack(sentence2sequence(row[\"sentence1\"].lower())[\"rows\"]))\n",
    "                hypothesis_embeds.append(\n",
    "                    np.vstack(sentence2sequence(row[\"sentence2\"].lower())[\"rows\"]))\n",
    "                labels.append(row[\"gold_label\"])\n",
    "                scores.append(sentence_score_setup(row))\n",
    "                print(\"Sample data piece: {}\".format(pp.pformat(row)))\n",
    "                # print(pp.pformat(sentence2sequence(row['sentence1'].lower())['rows']))\n",
    "                # print(np.vstack(pp.pformat(sentence2sequence(row['sentence1'].lower())['rows'])))\n",
    "                # print(pp.pformat(sentence_score_setup(row)))\n",
    "                break\n",
    "            premise_embeds = np.stack([fit_to_size(x, (FLAGS['max_premise_length'], FLAGS['embedding_size']))\n",
    "                                      for x in premise_embeds])\n",
    "            hypothesis_embeds = np.stack([fit_to_size(x, (FLAGS['max_hypothesis_length'], FLAGS['embedding_size']))\n",
    "                                         for x in hypothesis_embeds])\n",
    "            \n",
    "            return (np.array(premise_embeds), np.array(hypothesis_embeds)), labels, scores        \n",
    "    \n",
    "load_glove_embeddings()\n",
    "# 550152 rows processed\n",
    "data_features_tuple, labels, scores = load_snli_data(FLAGS['snli_trainfilename'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_length = FLAGS['max_premise_length'] + FLAGS['max_hypothesis_length'] # first layer length\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(550152,)\n",
      "[196009 304376 231330 304232 421940 124056  51227   1164 386737 421021\n",
      "  83404  17689  35619 488592 362493 237388 134576  53548  34074 312369\n",
      " 443143 193800 513020  65164 531701  32125 245461 496864 352922 284100\n",
      " 407511 239138 397509 224205 363856 339736 143113 152375 238305  47237\n",
      " 140212 297843 349415 368655 124533 460977 408040 313762 183325 456792\n",
      " 455842 198291 353704 254073 440335   9509 336486 209511 175368 148314\n",
      " 386784  78051 191574 433823 500165 244361 403903  90774 371095  79094\n",
      " 102464 474293 108214 519147  14697 217546 275235  74941 137053 439646\n",
      " 371747 413758  85852 133124 404831  83049 517217 402674 206020 100245\n",
      " 481365 309041  74696 259249 353265 309733 152715 498222 258519 261915\n",
      " 392926 491256  15952 168110 354946 410538 264103 394077 101536 165094\n",
      " 402456 470900 205137 127180 402022 401769 508720 216047   9677 151715\n",
      " 189836   3574 151599 462690 179618 226335 234097 189862]\n"
     ]
    }
   ],
   "source": [
    "a = tf.Variable(tf.ones(shape=(128,30,50)), name=\"a\")\n",
    "b = tf.Variable(tf.zeros(shape=(128,30,50)), name=\"b\")\n",
    "c = tf.concat([a, b], 1)\n",
    "d = tf.transpose(c, [1,0,2])\n",
    "# print(d)\n",
    "e = tf.reshape(d, [-1, 50])\n",
    "# print(e)\n",
    "f = tf.split(e, 60)\n",
    "#print(pp.pformat(f))\n",
    "# print(tf.equal(tf.Variable(True), tf.Variable(True)))\n",
    "# print(np.array([[1,2,3], [1,2,3]]).shape[0])\n",
    "print(data_features_tuple[0].shape)\n",
    "print(np.random.randint(data_features_tuple[0].shape[0], size=128))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'batch_size'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-326a1d882664>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mbatch_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mFLAGS\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mpremise_len\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mFLAGS\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_premise_length\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mhypothesis_len\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mFLAGS\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_hypothesis_length\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mdimensions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mFLAGS\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0membedding_size\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'dict' object has no attribute 'batch_size'"
     ]
    }
   ],
   "source": [
    "batch_size = FLAGS.batch_size\n",
    "premise_len = FLAGS.max_premise_length\n",
    "hypothesis_len = FLAGS.max_hypothesis_length\n",
    "dimensions = FLAGS.embedding_size\n",
    "\n",
    "input_layer = tf.keras.Input()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

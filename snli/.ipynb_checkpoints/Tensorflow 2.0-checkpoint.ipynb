{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load Hyperparameters and global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import argparse\n",
    "import tqdm\n",
    "import logging\n",
    "import pprint # pretty print python objects\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# new additions\n",
    "\n",
    "\n",
    "FLAGS = {\n",
    "    \"config_file\": \"snli.config\",\n",
    "    \"buffer_size\": 10000,\n",
    "    \"max_data_items\": 50000,\n",
    "    \"snli_link\": \"https://nlp.stanford.edu/projects/snli/snli_1.0.zip\",\n",
    "    \"snli_zipfilename\": \"snli_1.0.zip\",\n",
    "    \"snli_trainfilename\": \"snli_1.0_train.txt\",\n",
    "    \"snli_validatefilename\": \"snli_1.0_dev.txt\",\n",
    "    \"snli_testfilename\": \"snli_1.0_test.txt\",\n",
    "    \"word_embeddings_link\": \"http://nlp.stanford.edu/data/glove.6B.zip\",\n",
    "    \"word_embeddings_zipfilename\": \"glove.6B.zip\",\n",
    "    \"word_embeddings_txtfilename\": \"glove.6B.50d.txt\",\n",
    "    \"max_premise_length\": 30,\n",
    "    \"max_hypothesis_length\": 30,\n",
    "    \"batch_size\": 128,\n",
    "    \"hidden_length\": 64,\n",
    "    \"embedding_size\": 50, # 50 dim embeddings\n",
    "    \"max_features\": 50000,\n",
    "    \"num_epochs\": 5\n",
    "}\n",
    "\n",
    "def create_logger():\n",
    "    log = logging.getLogger() # root logger\n",
    "    log.setLevel(logging.DEBUG)\n",
    "    formatter = logging.Formatter(fmt=\"%(asctime)s : %(levelname)s %(message)s\")\n",
    "    handler = logging.StreamHandler()\n",
    "    handler.setFormatter(formatter)\n",
    "    log.addHandler(handler)\n",
    "    return logging.getLogger()\n",
    "\n",
    "pp = pprint.PrettyPrinter(indent=2)\n",
    "logger = create_logger()\n",
    "glove_wordmap = {}\n",
    "glove_wordmap_size = 0\n",
    "\n",
    "print(pp.pformat(FLAGS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load dataset manually\n",
    "def prepare_snli_corpus():\n",
    "    snli_link = FLAGS['snli_link']\n",
    "    snli_zip_file = FLAGS['snli_zipfilename']\n",
    "    snli_train_file = FLAGS['snli_trainfilename']\n",
    "    snli_validate_file = FLAGS['snli_validatefilename']\n",
    "    snli_test_file = FLAGS['snli_testfilename']\n",
    "    \n",
    "    if (not os.path.isfile(snli_zip_file)):\n",
    "        print(\"Snli corpus not found. Downloading from site...\")\n",
    "        import urllib.request\n",
    "        # download glove zip file\n",
    "        urllib.request.urlretrieve(snli_link, snli_zip_file)\n",
    "    print(\"Snli corpus file already downloaded. Extracting...\")\n",
    "    # extract train, validate and test files\n",
    "    if (not os.path.isfile(snli_train_file)):\n",
    "        unzip_single_file(snli_zip_file, snli_train_file)\n",
    "        print(\"Extracted {}\\n\".format(snli_validate_file))\n",
    "    else:\n",
    "        print(\"{} already extracted.\\n\".format(snli_train_file))\n",
    "    if (not os.path.isfile(snli_validate_file)):\n",
    "        unzip_single_file(snli_zip_file, snli_validate_file)\n",
    "        print(\"Extracted {}\\n\".format(snli_validate_file))\n",
    "    else:\n",
    "        print(\"{} already extracted.\\n\".format(snli_validate_file))\n",
    "    if (not os.path.isfile(snli_test_file)):\n",
    "        unzip_single_file(snli_zip_file, snli_test_file)\n",
    "        print(\"Extracted {}\\n\".format(snli_test_file))\n",
    "    else:\n",
    "        print(\"{} already extracted.\\n\".format(snli_validate_file))\n",
    "    return\n",
    "\n",
    "def prepare_glove_embeddings():\n",
    "    glove_link = FLAGS['word_embeddings_link']\n",
    "    glove_zip_file = FLAGS['word_embeddings_zipfilename']\n",
    "    glove_text_file = FLAGS['word_embeddings_txtfilename']\n",
    "    \n",
    "    if (not os.path.isfile(glove_zip_file) and not os.path.isfile(glove_text_file)):\n",
    "        print(\"Glove embeddings not found. Downloading from site...\")\n",
    "        import urllib.request\n",
    "        # download glove zip file\n",
    "        urllib.request.urlretrieve(glove_link, glove_zip_file)\n",
    "        print(\"Glove embeddings file downloaded.\")\n",
    "        # extract zip to text file\n",
    "        unzip_single_file(glove_zip_file, glove_text_file)\n",
    "    return\n",
    "\n",
    "def unzip_single_file(zip_file_name, output_file_name):\n",
    "    \"\"\"\n",
    "    If the outfile exists, don't recreate, else create from zipfile\n",
    "    \"\"\"\n",
    "    if not os.path.isfile(output_file_name):\n",
    "        import zipfile\n",
    "        print(\"Unzipping glove embeddings {}..\".format(zip_file_name))\n",
    "        with open(output_file_name, \"wb\") as out_file:\n",
    "            with zipfile.ZipFile(zip_file_name) as zipped:\n",
    "                for info in zipped.infolist():\n",
    "                    if output_file_name in info.filename:\n",
    "                        with zipped.open(info) as requested_file:\n",
    "                            out_file.write(requested_file.read())\n",
    "                            print(\"Glove embeddings unzipped to {}\".format(output_file_name))\n",
    "                            return\n",
    "    return\n",
    "\n",
    "prepare_snli_corpus()\n",
    "prepare_glove_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def sentence2sequence(sentence):\n",
    "    '''\n",
    "    Turns an input sentence into a (n, d) matrix. \n",
    "    n is the number of tokens in the sentence.\n",
    "    d is the number of dimensions each word vector has.\n",
    "    '''\n",
    "    tokens = None\n",
    "\n",
    "    try:\n",
    "        tokens = sentence.decode().lower().split(\" \")\n",
    "    except AttributeError: # not byte-encoded\n",
    "        tokens = sentence.lower().split(\" \")\n",
    "    rows = []\n",
    "    words = []\n",
    "    for token in tokens: # each token is a word in the sentence\n",
    "        i = len(token)\n",
    "        while len(token) > 0 and i > 0:\n",
    "            word = token[:i]\n",
    "            if word in glove_wordmap:\n",
    "                rows.append(glove_wordmap[word])\n",
    "                words.append(word)\n",
    "                token = token[i:]\n",
    "                i = len(token)\n",
    "            else:\n",
    "                # no such word, add keep reducing until we find a word\n",
    "                i = i - 1\n",
    "    return { \"words\": words, \"rows\": rows }\n",
    "\n",
    "def sentence_score_setup(row):\n",
    "    convert_dict = {\n",
    "        'contradiction': 0,\n",
    "        'neutral': 1,\n",
    "        'entailment': 2\n",
    "    }\n",
    "    score = np.zeros((3,1))\n",
    "    for x in range(1,6):\n",
    "        tag = row[\"label\"+str(x)]\n",
    "        if tag in convert_dict: \n",
    "            score[convert_dict[tag]] += 1\n",
    "    return score / (1.0 * np.sum(score)) # return normalised np array\n",
    "\n",
    "def fit_to_size(matrix, shape):\n",
    "    res = np.zeros(shape)\n",
    "    #print(\"Before: {}\".format(pp.pformat(matrix.shape)))\n",
    "    slices = [slice(0, min(dim, shape[e])) for e, dim in enumerate(matrix.shape)]\n",
    "    res[slices] = matrix[slices]\n",
    "    #print(\"After: {}\".format(pp.pformat(res.shape)))\n",
    "    return res\n",
    "\n",
    "def load_glove_embeddings():\n",
    "    global glove_wordmap\n",
    "    global glove_wordmap_size\n",
    "\n",
    "    glove_text_file = FLAGS['word_embeddings_txtfilename']\n",
    "    printOne = True    \n",
    "\n",
    "    with open(glove_text_file, \"r\", encoding='utf-8') as glove:\n",
    "        for line in glove:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            # tensorflow only accepts arrays, not python lists\n",
    "            featuresMatrix = np.asarray(values[1:], dtype='float32')\n",
    "            # print a sample word with feature matrix\n",
    "            if printOne:\n",
    "                printOne = False\n",
    "                print(\"Sample word \\\"{}\\\" with features {}\".format(word, pp.pformat(featuresMatrix)))\n",
    "            glove_wordmap[word] = featuresMatrix\n",
    "    glove_wordmap_size = len(glove_wordmap)\n",
    "    print(\"Glove wordmap populated, found %s vectors\\n\" % glove_wordmap_size)\n",
    "    \n",
    "def load_snli_data(filename):\n",
    "    if not os.path.isfile(filename):\n",
    "        print(\"ERROR: FILE NOT FOUND. EXITING...\")\n",
    "    else:\n",
    "        print(\"Preprocessing {} & parsing to arrays...\\n\".format(filename))\n",
    "        import csv\n",
    "        \n",
    "        convert_dict = { 'contradiction': 0, 'neutral': 1, 'entailment': 2 }\n",
    "        with open(filename, \"r\", encoding='utf-8') as data:\n",
    "            train = csv.DictReader(data, delimiter='\\t')\n",
    "            premise_embeds = []\n",
    "            hypothesis_embeds = []\n",
    "            labels = []\n",
    "            # array of binary class matrix e.g [1.0, 0.0, 0.0]\n",
    "            y = [] \n",
    "            i = 0\n",
    "            for row in tqdm.tqdm(iterable=train):\n",
    "                i += 1\n",
    "                if i > FLAGS['max_data_items']:\n",
    "                    break\n",
    "                premise_sentences.append(row[\"sentence1\"].lower())\n",
    "                hypothesis_sentences.append(row[\"sentence2\"].lower())\n",
    "                labels.append(row[\"gold_label\"])\n",
    "                y.append(convert_dict[row[\"gold_label\"]])\n",
    "                # print(\"Sample data piece: {}\".format(pp.pformat(row)))\n",
    "                # print(pp.pformat(sentence2sequence(row['sentence1'].lower())['rows']))\n",
    "                # print(np.vstack(pp.pformat(sentence2sequence(row['sentence1'].lower())['rows'])))\n",
    "                # print(pp.pformat(sentence_score_setup(row)))\n",
    "            return (premise_embeds_pad, hypothesis_embeds_pad), labels, np.array(scores)        \n",
    "    \n",
    "load_glove_embeddings()\n",
    "# max_data_items rows processed\n",
    "data_features_tuple, labels, scores = load_snli_data(FLAGS['snli_trainfilename'])\n",
    "v_features_tuple, v_labels, v_scores = load_snli_data(FLAGS['snli_validatefilename'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# a = tf.Variable(tf.ones(shape=(128,30,50)), name=\"a\")\n",
    "# b = tf.Variable(tf.zeros(shape=(128,30,50)), name=\"b\")\n",
    "# c = tf.concat([a, b], 1)\n",
    "# d = tf.transpose(c, [1,0,2])\n",
    "# print(d)\n",
    "# e = tf.reshape(d, [-1, 50])\n",
    "# print(e)\n",
    "# f = tf.split(e, 60)\n",
    "# print(pp.pformat(f))\n",
    "# print(tf.equal(tf.Variable(True), tf.Variable(True)))\n",
    "# print(np.array([[1,2,3], [1,2,3]]).shape[0])\n",
    "# print(data_features_tuple[0].shape)\n",
    "# print(np.random.randint(data_features_tuple[0].shape[0], size=128))\n",
    "\n",
    "# input_length = FLAGS['max_premise_length'] + FLAGS['max_hypothesis_length'] # first layer length\n",
    "# batch_size = FLAGS['batch_size']\n",
    "# premise_len = FLAGS['max_premise_length']\n",
    "# hypothesis_len = FLAGS['max_hypothesis_length']\n",
    "# dim = FLAGS['embedding_size']\n",
    "# hidden_len = FLAGS['hidden_length']\n",
    "# num_epochs = FLAGS['num_epochs']\n",
    "# max_data_items = FLAGS['max_data_items']\n",
    "\n",
    "# LABELS = {\n",
    "#     'entailment': 0,\n",
    "#     'neutral': 1,\n",
    "#     'contradiction': 2\n",
    "# }\n",
    "# print(data_features_tuple[0].shape)\n",
    "# premise_embeds_tensor = tf.Variable(data_features_tuple[0]) \n",
    "# hypothesis_embeds_tensor = tf.Variable(data_features_tuple[1])\n",
    "# print(premise_embeds_tensor.shape)\n",
    "# x = tf.keras.layers.concatenate([premise_embeds_tensor, hypothesis_embeds_tensor], axis=1)\n",
    "# print(x.shape)\n",
    "\n",
    "# (max_inputs, 60, 50)\n",
    "# input_tensor = layers.concatenate([data_features_tuple[0], data_features_tuple[1]], axis=1)\n",
    "# input_array = np.concatenate((data_features_tuple[0], data_features_tuple[1]), axis=1)\n",
    "# validate_tensor = layers.concatenate([v_features_tuple[0], v_features_tuple[1]], axis=1)\n",
    "# validate_array = np.concatenate((v_features_tuple[0], v_features_tuple[1]), axis=1)\n",
    "\n",
    "# model = keras.Sequential()\n",
    "# model.add(layers.Bidirectional(layers.LSTM(input_length, return_sequences=True), input_shape=(60, dim)))\n",
    "# model.add(layers.Dense(hidden_len, activation=\"relu\"))\n",
    "# model.add(layers.Dense(len(LABELS), activation=\"softmax\"))\n",
    "# model.compile(optimizer=\"rmsprop\", loss=\"sparse_categorical_crossentropy\", metrics=['accuracy'])\n",
    "# model.summary()\n",
    "# print(len(input_array))\n",
    "# print(len(scores))\n",
    "\n",
    "# model.fit(x=input_array, y=scores, batch_size=128, epochs=num_epochs, validation_data=(validate_array, v_scores), verbose=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
